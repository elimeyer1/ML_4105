{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elimeyer1/ML_4105/blob/main/Homework_7_Quesiton_1b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yFQcGmH27s9b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "040iBWr6_4oA"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rzWauVoM_9IT"
      },
      "outputs": [],
      "source": [
        "num_epochs = 200\n",
        "batch_size = 64\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BjVCqiA-__8E"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRzOgOdhAEFB",
        "outputId": "e4080f05-0371-4f5f-b342-7506e7398685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 72.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RjEIq6D5AHV1"
      },
      "outputs": [],
      "source": [
        "class ExtendedCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ExtendedCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(64 * 4 * 4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Mx2nw8j0CIlw"
      },
      "outputs": [],
      "source": [
        "model = ExtendedCNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3NMB3eEUDIgB"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "87UWi_QrDFgx"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
        "    train_losses, test_losses = [], []\n",
        "    train_accuracies, test_accuracies = [], []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0, 0, 0\n",
        "\n",
        "        for data, targets in train_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += targets.size(0)\n",
        "            train_correct += (predicted == targets).sum().item()\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        test_loss, test_correct, test_total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for data, targets in test_loader:\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                test_total += targets.size(0)\n",
        "                test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        test_acc = 100 * test_correct / test_total\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        test_losses.append(test_loss / len(test_loader))\n",
        "        train_accuracies.append(train_acc)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Loss {train_losses[-1]:.4f}, Test Acc {test_acc:.2f}%')\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    return training_time, train_losses[-1], test_losses[-1], train_accuracies[-1], test_accuracies[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqTY7Jv0DRpY",
        "outputId": "76d9534e-e62a-4837-ca72-a4bb2b7a9a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss 1.6042, Test Acc 52.33%\n",
            "Epoch 2: Train Loss 1.2547, Test Acc 61.91%\n",
            "Epoch 3: Train Loss 1.0886, Test Acc 65.87%\n",
            "Epoch 4: Train Loss 0.9763, Test Acc 67.26%\n",
            "Epoch 5: Train Loss 0.9000, Test Acc 68.71%\n",
            "Epoch 6: Train Loss 0.8308, Test Acc 70.02%\n",
            "Epoch 7: Train Loss 0.7843, Test Acc 71.70%\n",
            "Epoch 8: Train Loss 0.7402, Test Acc 71.99%\n",
            "Epoch 9: Train Loss 0.6987, Test Acc 72.03%\n",
            "Epoch 10: Train Loss 0.6571, Test Acc 73.28%\n",
            "Epoch 11: Train Loss 0.6303, Test Acc 73.68%\n",
            "Epoch 12: Train Loss 0.5946, Test Acc 73.17%\n",
            "Epoch 13: Train Loss 0.5705, Test Acc 73.24%\n",
            "Epoch 14: Train Loss 0.5518, Test Acc 73.31%\n",
            "Epoch 15: Train Loss 0.5256, Test Acc 73.90%\n",
            "Epoch 16: Train Loss 0.5028, Test Acc 74.07%\n",
            "Epoch 17: Train Loss 0.4864, Test Acc 73.84%\n",
            "Epoch 18: Train Loss 0.4608, Test Acc 73.57%\n",
            "Epoch 19: Train Loss 0.4470, Test Acc 74.42%\n",
            "Epoch 20: Train Loss 0.4315, Test Acc 73.74%\n",
            "Epoch 21: Train Loss 0.4229, Test Acc 73.49%\n",
            "Epoch 22: Train Loss 0.4020, Test Acc 73.82%\n",
            "Epoch 23: Train Loss 0.3920, Test Acc 73.70%\n",
            "Epoch 24: Train Loss 0.3772, Test Acc 74.44%\n",
            "Epoch 25: Train Loss 0.3740, Test Acc 74.07%\n",
            "Epoch 26: Train Loss 0.3595, Test Acc 74.01%\n",
            "Epoch 27: Train Loss 0.3469, Test Acc 74.08%\n",
            "Epoch 28: Train Loss 0.3418, Test Acc 73.93%\n",
            "Epoch 29: Train Loss 0.3292, Test Acc 74.27%\n",
            "Epoch 30: Train Loss 0.3273, Test Acc 74.07%\n",
            "Epoch 31: Train Loss 0.3213, Test Acc 73.99%\n",
            "Epoch 32: Train Loss 0.3099, Test Acc 73.55%\n",
            "Epoch 33: Train Loss 0.3125, Test Acc 73.98%\n",
            "Epoch 34: Train Loss 0.3037, Test Acc 73.25%\n",
            "Epoch 35: Train Loss 0.2915, Test Acc 74.13%\n",
            "Epoch 36: Train Loss 0.2880, Test Acc 73.22%\n",
            "Epoch 37: Train Loss 0.2831, Test Acc 73.83%\n",
            "Epoch 38: Train Loss 0.2802, Test Acc 74.31%\n",
            "Epoch 39: Train Loss 0.2798, Test Acc 73.86%\n",
            "Epoch 40: Train Loss 0.2697, Test Acc 74.06%\n",
            "Epoch 41: Train Loss 0.2663, Test Acc 73.32%\n",
            "Epoch 42: Train Loss 0.2598, Test Acc 74.06%\n",
            "Epoch 43: Train Loss 0.2586, Test Acc 74.26%\n",
            "Epoch 44: Train Loss 0.2656, Test Acc 73.79%\n",
            "Epoch 45: Train Loss 0.2646, Test Acc 72.33%\n",
            "Epoch 46: Train Loss 0.2528, Test Acc 73.63%\n",
            "Epoch 47: Train Loss 0.2457, Test Acc 73.22%\n",
            "Epoch 48: Train Loss 0.2435, Test Acc 73.65%\n",
            "Epoch 49: Train Loss 0.2426, Test Acc 73.95%\n",
            "Epoch 50: Train Loss 0.2423, Test Acc 73.48%\n",
            "Epoch 51: Train Loss 0.2397, Test Acc 73.90%\n",
            "Epoch 52: Train Loss 0.2340, Test Acc 73.98%\n",
            "Epoch 53: Train Loss 0.2265, Test Acc 73.51%\n",
            "Epoch 54: Train Loss 0.2275, Test Acc 73.72%\n",
            "Epoch 55: Train Loss 0.2332, Test Acc 73.55%\n",
            "Epoch 56: Train Loss 0.2283, Test Acc 73.89%\n",
            "Epoch 57: Train Loss 0.2279, Test Acc 73.44%\n",
            "Epoch 58: Train Loss 0.2202, Test Acc 73.46%\n",
            "Epoch 59: Train Loss 0.2213, Test Acc 73.81%\n",
            "Epoch 60: Train Loss 0.2263, Test Acc 73.11%\n",
            "Epoch 61: Train Loss 0.2162, Test Acc 73.49%\n",
            "Epoch 62: Train Loss 0.2175, Test Acc 73.33%\n",
            "Epoch 63: Train Loss 0.2128, Test Acc 73.46%\n",
            "Epoch 64: Train Loss 0.2088, Test Acc 73.75%\n",
            "Epoch 65: Train Loss 0.2143, Test Acc 74.02%\n",
            "Epoch 66: Train Loss 0.2107, Test Acc 73.28%\n",
            "Epoch 67: Train Loss 0.2106, Test Acc 73.54%\n",
            "Epoch 68: Train Loss 0.2079, Test Acc 73.41%\n",
            "Epoch 69: Train Loss 0.2115, Test Acc 73.37%\n",
            "Epoch 70: Train Loss 0.2045, Test Acc 73.97%\n",
            "Epoch 71: Train Loss 0.2027, Test Acc 73.38%\n",
            "Epoch 72: Train Loss 0.2046, Test Acc 73.49%\n",
            "Epoch 73: Train Loss 0.2029, Test Acc 73.69%\n",
            "Epoch 74: Train Loss 0.1924, Test Acc 73.63%\n",
            "Epoch 75: Train Loss 0.2017, Test Acc 73.72%\n",
            "Epoch 76: Train Loss 0.1967, Test Acc 73.27%\n",
            "Epoch 77: Train Loss 0.2009, Test Acc 73.57%\n",
            "Epoch 78: Train Loss 0.1945, Test Acc 73.60%\n",
            "Epoch 79: Train Loss 0.1932, Test Acc 73.58%\n",
            "Epoch 80: Train Loss 0.1915, Test Acc 73.64%\n",
            "Epoch 81: Train Loss 0.1900, Test Acc 73.32%\n",
            "Epoch 82: Train Loss 0.1971, Test Acc 73.43%\n",
            "Epoch 83: Train Loss 0.1854, Test Acc 73.95%\n",
            "Epoch 84: Train Loss 0.1933, Test Acc 73.89%\n",
            "Epoch 85: Train Loss 0.1881, Test Acc 73.84%\n",
            "Epoch 86: Train Loss 0.1892, Test Acc 73.32%\n",
            "Epoch 87: Train Loss 0.1891, Test Acc 73.65%\n",
            "Epoch 88: Train Loss 0.1879, Test Acc 73.81%\n",
            "Epoch 89: Train Loss 0.1839, Test Acc 73.40%\n",
            "Epoch 90: Train Loss 0.1905, Test Acc 73.69%\n",
            "Epoch 91: Train Loss 0.1838, Test Acc 73.26%\n",
            "Epoch 92: Train Loss 0.1822, Test Acc 73.66%\n",
            "Epoch 93: Train Loss 0.1835, Test Acc 74.50%\n",
            "Epoch 94: Train Loss 0.1778, Test Acc 73.89%\n",
            "Epoch 95: Train Loss 0.1802, Test Acc 73.26%\n",
            "Epoch 96: Train Loss 0.1796, Test Acc 73.79%\n",
            "Epoch 97: Train Loss 0.1843, Test Acc 73.81%\n",
            "Epoch 98: Train Loss 0.1827, Test Acc 73.50%\n",
            "Epoch 99: Train Loss 0.1772, Test Acc 73.77%\n",
            "Epoch 100: Train Loss 0.1824, Test Acc 73.75%\n",
            "Epoch 101: Train Loss 0.1702, Test Acc 73.35%\n",
            "Epoch 102: Train Loss 0.1735, Test Acc 74.13%\n",
            "Epoch 103: Train Loss 0.1850, Test Acc 73.83%\n",
            "Epoch 104: Train Loss 0.1722, Test Acc 73.74%\n",
            "Epoch 105: Train Loss 0.1723, Test Acc 74.39%\n",
            "Epoch 106: Train Loss 0.1801, Test Acc 74.23%\n",
            "Epoch 107: Train Loss 0.1736, Test Acc 73.94%\n",
            "Epoch 108: Train Loss 0.1687, Test Acc 73.70%\n",
            "Epoch 109: Train Loss 0.1670, Test Acc 74.24%\n",
            "Epoch 110: Train Loss 0.1730, Test Acc 73.79%\n",
            "Epoch 111: Train Loss 0.1649, Test Acc 73.84%\n",
            "Epoch 112: Train Loss 0.1664, Test Acc 73.71%\n",
            "Epoch 113: Train Loss 0.1673, Test Acc 74.01%\n",
            "Epoch 114: Train Loss 0.1739, Test Acc 73.65%\n",
            "Epoch 115: Train Loss 0.1655, Test Acc 73.63%\n",
            "Epoch 116: Train Loss 0.1765, Test Acc 73.73%\n",
            "Epoch 117: Train Loss 0.1632, Test Acc 73.67%\n",
            "Epoch 118: Train Loss 0.1636, Test Acc 73.90%\n",
            "Epoch 119: Train Loss 0.1719, Test Acc 73.45%\n",
            "Epoch 120: Train Loss 0.1657, Test Acc 73.49%\n",
            "Epoch 121: Train Loss 0.1664, Test Acc 73.45%\n",
            "Epoch 122: Train Loss 0.1632, Test Acc 73.70%\n",
            "Epoch 123: Train Loss 0.1612, Test Acc 73.61%\n",
            "Epoch 124: Train Loss 0.1692, Test Acc 73.47%\n",
            "Epoch 125: Train Loss 0.1567, Test Acc 73.58%\n",
            "Epoch 126: Train Loss 0.1663, Test Acc 73.19%\n",
            "Epoch 127: Train Loss 0.1607, Test Acc 72.98%\n",
            "Epoch 128: Train Loss 0.1694, Test Acc 73.68%\n",
            "Epoch 129: Train Loss 0.1596, Test Acc 73.58%\n",
            "Epoch 130: Train Loss 0.1626, Test Acc 73.76%\n",
            "Epoch 131: Train Loss 0.1577, Test Acc 73.73%\n",
            "Epoch 132: Train Loss 0.1555, Test Acc 73.78%\n",
            "Epoch 133: Train Loss 0.1690, Test Acc 73.56%\n",
            "Epoch 134: Train Loss 0.1651, Test Acc 73.33%\n",
            "Epoch 135: Train Loss 0.1485, Test Acc 73.44%\n",
            "Epoch 136: Train Loss 0.1641, Test Acc 73.34%\n",
            "Epoch 137: Train Loss 0.1560, Test Acc 73.39%\n",
            "Epoch 138: Train Loss 0.1536, Test Acc 73.81%\n",
            "Epoch 139: Train Loss 0.1539, Test Acc 73.16%\n",
            "Epoch 140: Train Loss 0.1589, Test Acc 73.65%\n",
            "Epoch 141: Train Loss 0.1608, Test Acc 73.71%\n",
            "Epoch 142: Train Loss 0.1479, Test Acc 73.59%\n",
            "Epoch 143: Train Loss 0.1640, Test Acc 73.29%\n",
            "Epoch 144: Train Loss 0.1556, Test Acc 73.34%\n",
            "Epoch 145: Train Loss 0.1485, Test Acc 73.97%\n",
            "Epoch 146: Train Loss 0.1517, Test Acc 72.78%\n",
            "Epoch 147: Train Loss 0.1609, Test Acc 73.92%\n",
            "Epoch 148: Train Loss 0.1468, Test Acc 73.60%\n",
            "Epoch 149: Train Loss 0.1562, Test Acc 74.12%\n",
            "Epoch 150: Train Loss 0.1486, Test Acc 73.36%\n",
            "Epoch 151: Train Loss 0.1580, Test Acc 73.46%\n",
            "Epoch 152: Train Loss 0.1516, Test Acc 73.68%\n",
            "Epoch 153: Train Loss 0.1547, Test Acc 73.40%\n",
            "Epoch 154: Train Loss 0.1531, Test Acc 72.90%\n",
            "Epoch 155: Train Loss 0.1447, Test Acc 73.47%\n",
            "Epoch 156: Train Loss 0.1556, Test Acc 73.78%\n",
            "Epoch 157: Train Loss 0.1505, Test Acc 73.07%\n",
            "Epoch 158: Train Loss 0.1532, Test Acc 73.50%\n",
            "Epoch 159: Train Loss 0.1521, Test Acc 73.85%\n",
            "Epoch 160: Train Loss 0.1503, Test Acc 73.40%\n",
            "Epoch 161: Train Loss 0.1544, Test Acc 73.77%\n",
            "Epoch 162: Train Loss 0.1457, Test Acc 73.42%\n",
            "Epoch 163: Train Loss 0.1494, Test Acc 73.44%\n",
            "Epoch 164: Train Loss 0.1540, Test Acc 73.84%\n",
            "Epoch 165: Train Loss 0.1472, Test Acc 73.53%\n",
            "Epoch 166: Train Loss 0.1422, Test Acc 73.83%\n",
            "Epoch 167: Train Loss 0.1472, Test Acc 73.07%\n",
            "Epoch 168: Train Loss 0.1523, Test Acc 73.65%\n",
            "Epoch 169: Train Loss 0.1397, Test Acc 73.59%\n",
            "Epoch 170: Train Loss 0.1476, Test Acc 73.00%\n",
            "Epoch 171: Train Loss 0.1450, Test Acc 73.63%\n",
            "Epoch 172: Train Loss 0.1465, Test Acc 73.44%\n",
            "Epoch 173: Train Loss 0.1363, Test Acc 73.55%\n",
            "Epoch 174: Train Loss 0.1480, Test Acc 73.51%\n",
            "Epoch 175: Train Loss 0.1499, Test Acc 73.65%\n",
            "Epoch 176: Train Loss 0.1459, Test Acc 73.69%\n",
            "Epoch 177: Train Loss 0.1485, Test Acc 73.59%\n",
            "Epoch 178: Train Loss 0.1479, Test Acc 73.72%\n",
            "Epoch 179: Train Loss 0.1477, Test Acc 73.99%\n",
            "Epoch 180: Train Loss 0.1480, Test Acc 74.01%\n",
            "Epoch 181: Train Loss 0.1394, Test Acc 73.69%\n",
            "Epoch 182: Train Loss 0.1454, Test Acc 73.75%\n",
            "Epoch 183: Train Loss 0.1431, Test Acc 73.59%\n",
            "Epoch 184: Train Loss 0.1376, Test Acc 73.80%\n",
            "Epoch 185: Train Loss 0.1432, Test Acc 73.90%\n",
            "Epoch 186: Train Loss 0.1463, Test Acc 73.36%\n",
            "Epoch 187: Train Loss 0.1461, Test Acc 73.45%\n",
            "Epoch 188: Train Loss 0.1436, Test Acc 74.13%\n",
            "Epoch 189: Train Loss 0.1438, Test Acc 73.84%\n",
            "Epoch 190: Train Loss 0.1361, Test Acc 73.86%\n",
            "Epoch 191: Train Loss 0.1435, Test Acc 73.10%\n",
            "Epoch 192: Train Loss 0.1414, Test Acc 72.98%\n",
            "Epoch 193: Train Loss 0.1461, Test Acc 73.41%\n",
            "Epoch 194: Train Loss 0.1400, Test Acc 73.47%\n",
            "Epoch 195: Train Loss 0.1405, Test Acc 73.91%\n",
            "Epoch 196: Train Loss 0.1420, Test Acc 73.50%\n",
            "Epoch 197: Train Loss 0.1447, Test Acc 73.49%\n",
            "Epoch 198: Train Loss 0.1435, Test Acc 73.50%\n",
            "Epoch 199: Train Loss 0.1418, Test Acc 72.79%\n",
            "Epoch 200: Train Loss 0.1403, Test Acc 72.90%\n"
          ]
        }
      ],
      "source": [
        "training_time, train_loss, test_loss, train_acc, test_acc = train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
        "\n",
        "model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "utcca6B9s1Wx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b197eb6f-ce69-436f-e1da-0263b250f354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size: 288554 parameters\n",
            "Training Time: 13430.22 seconds\n"
          ]
        }
      ],
      "source": [
        "print(f'Model Size: {model_size} parameters')\n",
        "print(f'Training Time: {training_time:.2f} seconds')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4ucrl/kqnPMv6YWZQFYKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
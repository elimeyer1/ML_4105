{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elimeyer1/ML_4105/blob/main/Homework_7_Quesiton_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MrM5W-ft-Jlk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lFAalhaaIKB4"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fxCttpOTIOAx"
      },
      "outputs": [],
      "source": [
        "num_epochs = 200\n",
        "batch_size = 64\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qki2f4MCId0b"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyxOlAYoIgKK",
        "outputId": "bfe5dfab-8592-4413-f365-e86739569315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 79.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4r2-piB6JQr2"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eTJFR7OMJTct"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(32 * 8 * 8, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_iJPTW7jsqUq"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XZ7btPySstFB"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QvDoPE7-svqk"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    return training_time, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pn1rasfTsyVn"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JhGQzaPszBi",
        "outputId": "b7437c76-7a8f-4f7f-d568-80b46bf59e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Loss: 1.4345\n",
            "Epoch [2/200], Loss: 1.0931\n",
            "Epoch [3/200], Loss: 0.9440\n",
            "Epoch [4/200], Loss: 0.8409\n",
            "Epoch [5/200], Loss: 0.7603\n",
            "Epoch [6/200], Loss: 0.6924\n",
            "Epoch [7/200], Loss: 0.6264\n",
            "Epoch [8/200], Loss: 0.5627\n",
            "Epoch [9/200], Loss: 0.5026\n",
            "Epoch [10/200], Loss: 0.4445\n",
            "Epoch [11/200], Loss: 0.3989\n",
            "Epoch [12/200], Loss: 0.3456\n",
            "Epoch [13/200], Loss: 0.3036\n",
            "Epoch [14/200], Loss: 0.2577\n",
            "Epoch [15/200], Loss: 0.2225\n",
            "Epoch [16/200], Loss: 0.1948\n",
            "Epoch [17/200], Loss: 0.1607\n",
            "Epoch [18/200], Loss: 0.1474\n",
            "Epoch [19/200], Loss: 0.1276\n",
            "Epoch [20/200], Loss: 0.1115\n",
            "Epoch [21/200], Loss: 0.1041\n",
            "Epoch [22/200], Loss: 0.0882\n",
            "Epoch [23/200], Loss: 0.0952\n",
            "Epoch [24/200], Loss: 0.0745\n",
            "Epoch [25/200], Loss: 0.0719\n",
            "Epoch [26/200], Loss: 0.0860\n",
            "Epoch [27/200], Loss: 0.0653\n",
            "Epoch [28/200], Loss: 0.0668\n",
            "Epoch [29/200], Loss: 0.0662\n",
            "Epoch [30/200], Loss: 0.0639\n",
            "Epoch [31/200], Loss: 0.0592\n",
            "Epoch [32/200], Loss: 0.0590\n",
            "Epoch [33/200], Loss: 0.0519\n",
            "Epoch [34/200], Loss: 0.0607\n",
            "Epoch [35/200], Loss: 0.0528\n",
            "Epoch [36/200], Loss: 0.0601\n",
            "Epoch [37/200], Loss: 0.0501\n",
            "Epoch [38/200], Loss: 0.0514\n",
            "Epoch [39/200], Loss: 0.0505\n",
            "Epoch [40/200], Loss: 0.0503\n",
            "Epoch [41/200], Loss: 0.0492\n",
            "Epoch [42/200], Loss: 0.0501\n",
            "Epoch [43/200], Loss: 0.0375\n",
            "Epoch [44/200], Loss: 0.0457\n",
            "Epoch [45/200], Loss: 0.0581\n",
            "Epoch [46/200], Loss: 0.0520\n",
            "Epoch [47/200], Loss: 0.0340\n",
            "Epoch [48/200], Loss: 0.0480\n",
            "Epoch [49/200], Loss: 0.0376\n",
            "Epoch [50/200], Loss: 0.0499\n",
            "Epoch [51/200], Loss: 0.0356\n",
            "Epoch [52/200], Loss: 0.0520\n",
            "Epoch [53/200], Loss: 0.0297\n",
            "Epoch [54/200], Loss: 0.0472\n",
            "Epoch [55/200], Loss: 0.0402\n",
            "Epoch [56/200], Loss: 0.0463\n",
            "Epoch [57/200], Loss: 0.0396\n",
            "Epoch [58/200], Loss: 0.0308\n",
            "Epoch [59/200], Loss: 0.0398\n",
            "Epoch [60/200], Loss: 0.0463\n",
            "Epoch [61/200], Loss: 0.0289\n",
            "Epoch [62/200], Loss: 0.0439\n",
            "Epoch [63/200], Loss: 0.0355\n",
            "Epoch [64/200], Loss: 0.0337\n",
            "Epoch [65/200], Loss: 0.0412\n",
            "Epoch [66/200], Loss: 0.0359\n",
            "Epoch [67/200], Loss: 0.0343\n",
            "Epoch [68/200], Loss: 0.0357\n",
            "Epoch [69/200], Loss: 0.0326\n",
            "Epoch [70/200], Loss: 0.0341\n",
            "Epoch [71/200], Loss: 0.0401\n",
            "Epoch [72/200], Loss: 0.0283\n",
            "Epoch [73/200], Loss: 0.0457\n",
            "Epoch [74/200], Loss: 0.0347\n",
            "Epoch [75/200], Loss: 0.0277\n",
            "Epoch [76/200], Loss: 0.0336\n",
            "Epoch [77/200], Loss: 0.0386\n",
            "Epoch [78/200], Loss: 0.0332\n",
            "Epoch [79/200], Loss: 0.0271\n",
            "Epoch [80/200], Loss: 0.0306\n",
            "Epoch [81/200], Loss: 0.0370\n",
            "Epoch [82/200], Loss: 0.0247\n",
            "Epoch [83/200], Loss: 0.0402\n",
            "Epoch [84/200], Loss: 0.0331\n",
            "Epoch [85/200], Loss: 0.0263\n",
            "Epoch [86/200], Loss: 0.0397\n",
            "Epoch [87/200], Loss: 0.0226\n",
            "Epoch [88/200], Loss: 0.0356\n",
            "Epoch [89/200], Loss: 0.0296\n",
            "Epoch [90/200], Loss: 0.0259\n",
            "Epoch [91/200], Loss: 0.0324\n",
            "Epoch [92/200], Loss: 0.0284\n",
            "Epoch [93/200], Loss: 0.0374\n",
            "Epoch [94/200], Loss: 0.0310\n",
            "Epoch [95/200], Loss: 0.0274\n",
            "Epoch [96/200], Loss: 0.0297\n",
            "Epoch [97/200], Loss: 0.0243\n",
            "Epoch [98/200], Loss: 0.0315\n",
            "Epoch [99/200], Loss: 0.0376\n",
            "Epoch [100/200], Loss: 0.0291\n",
            "Epoch [101/200], Loss: 0.0313\n",
            "Epoch [102/200], Loss: 0.0256\n",
            "Epoch [103/200], Loss: 0.0301\n",
            "Epoch [104/200], Loss: 0.0299\n",
            "Epoch [105/200], Loss: 0.0238\n",
            "Epoch [106/200], Loss: 0.0398\n",
            "Epoch [107/200], Loss: 0.0251\n",
            "Epoch [108/200], Loss: 0.0317\n",
            "Epoch [109/200], Loss: 0.0233\n",
            "Epoch [110/200], Loss: 0.0357\n",
            "Epoch [111/200], Loss: 0.0291\n",
            "Epoch [112/200], Loss: 0.0275\n",
            "Epoch [113/200], Loss: 0.0258\n",
            "Epoch [114/200], Loss: 0.0338\n",
            "Epoch [115/200], Loss: 0.0333\n",
            "Epoch [116/200], Loss: 0.0242\n",
            "Epoch [117/200], Loss: 0.0182\n",
            "Epoch [118/200], Loss: 0.0322\n",
            "Epoch [119/200], Loss: 0.0256\n",
            "Epoch [120/200], Loss: 0.0274\n",
            "Epoch [121/200], Loss: 0.0275\n",
            "Epoch [122/200], Loss: 0.0298\n",
            "Epoch [123/200], Loss: 0.0183\n",
            "Epoch [124/200], Loss: 0.0297\n",
            "Epoch [125/200], Loss: 0.0315\n",
            "Epoch [126/200], Loss: 0.0279\n",
            "Epoch [127/200], Loss: 0.0193\n",
            "Epoch [128/200], Loss: 0.0303\n",
            "Epoch [129/200], Loss: 0.0301\n",
            "Epoch [130/200], Loss: 0.0225\n",
            "Epoch [131/200], Loss: 0.0242\n",
            "Epoch [132/200], Loss: 0.0294\n",
            "Epoch [133/200], Loss: 0.0268\n",
            "Epoch [134/200], Loss: 0.0206\n",
            "Epoch [135/200], Loss: 0.0287\n",
            "Epoch [136/200], Loss: 0.0279\n",
            "Epoch [137/200], Loss: 0.0289\n",
            "Epoch [138/200], Loss: 0.0243\n",
            "Epoch [139/200], Loss: 0.0286\n",
            "Epoch [140/200], Loss: 0.0273\n",
            "Epoch [141/200], Loss: 0.0212\n",
            "Epoch [142/200], Loss: 0.0263\n",
            "Epoch [143/200], Loss: 0.0270\n",
            "Epoch [144/200], Loss: 0.0217\n",
            "Epoch [145/200], Loss: 0.0241\n",
            "Epoch [146/200], Loss: 0.0294\n",
            "Epoch [147/200], Loss: 0.0307\n",
            "Epoch [148/200], Loss: 0.0245\n",
            "Epoch [149/200], Loss: 0.0242\n",
            "Epoch [150/200], Loss: 0.0218\n",
            "Epoch [151/200], Loss: 0.0365\n",
            "Epoch [152/200], Loss: 0.0219\n",
            "Epoch [153/200], Loss: 0.0254\n",
            "Epoch [154/200], Loss: 0.0225\n",
            "Epoch [155/200], Loss: 0.0189\n",
            "Epoch [156/200], Loss: 0.0216\n",
            "Epoch [157/200], Loss: 0.0199\n",
            "Epoch [158/200], Loss: 0.0316\n",
            "Epoch [159/200], Loss: 0.0223\n",
            "Epoch [160/200], Loss: 0.0245\n",
            "Epoch [161/200], Loss: 0.0268\n",
            "Epoch [162/200], Loss: 0.0281\n",
            "Epoch [163/200], Loss: 0.0285\n",
            "Epoch [164/200], Loss: 0.0202\n",
            "Epoch [165/200], Loss: 0.0260\n",
            "Epoch [166/200], Loss: 0.0303\n",
            "Epoch [167/200], Loss: 0.0307\n",
            "Epoch [168/200], Loss: 0.0190\n",
            "Epoch [169/200], Loss: 0.0223\n",
            "Epoch [170/200], Loss: 0.0281\n",
            "Epoch [171/200], Loss: 0.0235\n",
            "Epoch [172/200], Loss: 0.0259\n",
            "Epoch [173/200], Loss: 0.0257\n",
            "Epoch [174/200], Loss: 0.0221\n",
            "Epoch [175/200], Loss: 0.0205\n",
            "Epoch [176/200], Loss: 0.0244\n",
            "Epoch [177/200], Loss: 0.0269\n",
            "Epoch [178/200], Loss: 0.0251\n",
            "Epoch [179/200], Loss: 0.0259\n",
            "Epoch [180/200], Loss: 0.0226\n",
            "Epoch [181/200], Loss: 0.0220\n",
            "Epoch [182/200], Loss: 0.0320\n",
            "Epoch [183/200], Loss: 0.0201\n",
            "Epoch [184/200], Loss: 0.0217\n",
            "Epoch [185/200], Loss: 0.0274\n",
            "Epoch [186/200], Loss: 0.0229\n",
            "Epoch [187/200], Loss: 0.0207\n",
            "Epoch [188/200], Loss: 0.0255\n",
            "Epoch [189/200], Loss: 0.0226\n",
            "Epoch [190/200], Loss: 0.0277\n",
            "Epoch [191/200], Loss: 0.0202\n",
            "Epoch [192/200], Loss: 0.0252\n",
            "Epoch [193/200], Loss: 0.0180\n",
            "Epoch [194/200], Loss: 0.0309\n",
            "Epoch [195/200], Loss: 0.0241\n",
            "Epoch [196/200], Loss: 0.0204\n",
            "Epoch [197/200], Loss: 0.0281\n",
            "Epoch [198/200], Loss: 0.0178\n",
            "Epoch [199/200], Loss: 0.0193\n",
            "Epoch [200/200], Loss: 0.0296\n"
          ]
        }
      ],
      "source": [
        "training_time, training_loss = train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
        "accuracy = evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "utcca6B9s1Wx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ccde18b-1639-413f-ff34-d1e71ea7549b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size: 268650 parameters\n",
            "Training Time: 10706.91 seconds\n",
            "Training Loss: 0.0296\n",
            "Test Accuracy: 66.55%\n"
          ]
        }
      ],
      "source": [
        "model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Model Size: {model_size} parameters')\n",
        "print(f'Training Time: {training_time:.2f} seconds')\n",
        "print(f'Training Loss: {training_loss:.4f}')\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwpaMrovTmv2hIzkt2QJGP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}